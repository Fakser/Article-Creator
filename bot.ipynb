{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "from googlesearch import search\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from docx import Document\n",
    "import string\n",
    "import shutil\n",
    "import os\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from numpy.random import uniform\n",
    "import language_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO bardziej losowe strony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP and webscrapping based article/report creator\n",
    "# author: Krzysztof Kramarz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our hyperparameters\n",
    "topic = 'cute anime girls' # Topic of an article\n",
    "sentence_quality = 0.4 # variable describing quality of the sentence\n",
    "number_of_articles = 10 # number of articles scipt will be scrapping\n",
    "change_to_synonym_chance = 0.1\n",
    "max_word_len = 10\n",
    "tool = language_check.LanguageTool('en-US') # tool for checking grammar in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition:_Ripples_of_Adventure\nhttps://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012\nhttps://jw-webmagazine.com/hirohiko-araki-jojo-exhibition-ripples-of-adventure-is-coming-to-tokyo-and-osaka-in-2018-96a756d3dc6d/\nhttps://en.wikipedia.org/wiki/JoJo%27s_Bizarre_Adventure\nhttps://www.pinterest.com/pin/327073991686932797/\nhttps://www.pinterest.com/pin/585679126519367412/\nhttps://geekytravelsfandoms.com/2018/11/24/jojo-bizarre-adventure-2018-exhibition/\nhttps://www.amazon.com/Hirohiko-Araki-Original-Artwork-Exhibition/dp/B00B66N1LM\nhttps://www.ebay.com/i/164008848534?chn=ps\nhttps://www.fanfigs.com/en/product/21165/1st-payment-jojo-exhibition-giorno-giorvanna-brooch-jojo039s-bizarre-adventure-part-5-golden-wind\n_________________________________________\n"
    }
   ],
   "source": [
    "# getting urls of pages that will be scraped\n",
    "# first we will use googlesearch module for getting results from google\n",
    "google_search = search('everything about ' + topic)\n",
    "urls = []\n",
    "number_of_proper_articles = 0\n",
    "\n",
    "# we will go through these results as long as we will find enough different pages \n",
    "while number_of_proper_articles < number_of_articles:\n",
    "    # we get next url from the generator\n",
    "    new_url = next(google_search)\n",
    "    bad_url = False\n",
    "    # next we check if these page was not already added to the list (google likes to give you the same result many times)\n",
    "    for url in urls:\n",
    "        if url in new_url or new_url in url or 'youtube' in new_url:\n",
    "            bad_url = True\n",
    "            break\n",
    "    if bad_url == False:\n",
    "        urls.append(new_url)\n",
    "        number_of_proper_articles += 1\n",
    "        print(new_url)\n",
    "print('_________________________________________')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition:_Ripples_of_Adventure    Downloaded\nhttps://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012    Downloaded\nhttps://jw-webmagazine.com/hirohiko-araki-jojo-exhibition-ripples-of-adventure-is-coming-to-tokyo-and-osaka-in-2018-96a756d3dc6d/    Downloaded\nhttps://en.wikipedia.org/wiki/JoJo%27s_Bizarre_Adventure    Downloaded\nhttps://www.pinterest.com/pin/327073991686932797/    Downloaded\nhttps://www.pinterest.com/pin/585679126519367412/    Downloaded\nhttps://geekytravelsfandoms.com/2018/11/24/jojo-bizarre-adventure-2018-exhibition/    Downloaded\nhttps://www.amazon.com/Hirohiko-Araki-Original-Artwork-Exhibition/dp/B00B66N1LM    Downloaded\nhttps://www.ebay.com/i/164008848534?chn=ps    Downloaded\nhttps://www.fanfigs.com/en/product/21165/1st-payment-jojo-exhibition-giorno-giorvanna-brooch-jojo039s-bizarre-adventure-part-5-golden-wind    Downloaded\n"
    }
   ],
   "source": [
    "# next we will get response html with requests.get(url) and split it into text and images\n",
    "articles = []\n",
    "images = []\n",
    "for url in urls:\n",
    "    # downloading response \n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        print(url + '    Downloaded')\n",
    "    except:\n",
    "        print(url + '    Failed')\n",
    "        continue\n",
    "    # splitting response by bracets to get code part and text part separetly - i found this way the fastest and most proficient\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    page_splitted = re.split('{|}',soup.text)\n",
    "    for article in page_splitted:\n",
    "        # cleaning text part from unnecesary stuff\n",
    "        article = article.replace('\\n', ' ').replace('  ','')\n",
    "        new_article = ''\n",
    "        for sentence in article.split('. '):\n",
    "            new_article += re.sub('[\"#$%&()*+-/:;<=>@^_{|}~\\r\\t]', '', sentence) + '. '\n",
    "        # if this text meets our requierments we add it to the list of articles\n",
    "        if len(new_article) >= 4000:\n",
    "            articles.append(re.sub('(figure|fig|fig.) [0-9]', '' ,new_article))\n",
    "    # iterating throught all img objects found in responce to get some hot pictures\n",
    "    for index, image in enumerate(soup.findAll('img')):\n",
    "        if 'srcset' in image.attrs.keys():\n",
    "            images.append(image.attrs['srcset'].split(',')[0])\n",
    "        # if index > int(len(soup.findAll('img')) * 4/7) and index < int(len(soup.findAll('img')) * 5/7) and 'src' in image.attrs.keys():\n",
    "        #     images.append(image.attrs['src'])\n",
    "\n",
    "    sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading photos from the list of image urls\n",
    "# creating directory for photos\n",
    "os.mkdir('./photos')\n",
    "for index, image_url in enumerate(images):\n",
    "    # getting path for file\n",
    "    if '.png' in image_url:\n",
    "        file_type = '.png'\n",
    "    else:\n",
    "        file_type = '.jpg'\n",
    "    file_path = './photos/' + str(index) + 'bonus' + file_type\n",
    "    delete = False\n",
    "    # creating file from url with many conditions \n",
    "    with open(file_path, 'wb') as photo:\n",
    "        try:\n",
    "            photo.write(requests.get(image_url.split(' ')[0]).content)\n",
    "        except:\n",
    "            try:\n",
    "                photo.write(request.urlopen(image_url).read())\n",
    "            except:\n",
    "                delete = True\n",
    "                continue\n",
    "    try:\n",
    "        # checking if image is not corrupted (not really working yet) and resizing it so it can fit the document\n",
    "        im = Image.open(file_path)\n",
    "        im.verify()\n",
    "        im.close()\n",
    "        basewidth = 500\n",
    "        img = Image.open(file_path)\n",
    "        if img.size[0] > 500:\n",
    "            wpercent = (basewidth/float(img.size[0]))\n",
    "            hsize = int((float(img.size[1])*float(wpercent)))\n",
    "            img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "        img.save(file_path)\n",
    "        img.close()\n",
    "    except Exception as ex:\n",
    "        try:\n",
    "            os.remove('./photos/' + str(index) + 'bonus' + file_type)\n",
    "        except:\n",
    "            continue\n",
    "        print(ex) \n",
    "    if delete:\n",
    "        try:\n",
    "            os.remove('./photos/' + str(index) + 'bonus' + file_type)\n",
    "        except:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms_and_antonyms(word):\n",
    "    # function that returns list of synonyms ant antonyms for given word\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(wprd):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                    antonyms.append(l.antonyms()[0].name())\n",
    "    return synonyms, antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality of sentence metrics\n",
    "# we need to get only most sensical senetences with some gramarr cleaning and word replacements\n",
    "english_dictonary = set(words.words())\n",
    "cleaned_articles = [] \n",
    "for article in articles:\n",
    "    cleaned_article = []\n",
    "    # splitting article into setentces\n",
    "    for sentence in article.split('. '):\n",
    "        sentence_to_words = re.split(' ', sentence)\n",
    "        # measure of sentence sense and cleaning it\n",
    "        metrics = 0\n",
    "        new_sentence = ''\n",
    "        for word in sentence_to_words:\n",
    "            if word in english_dictonary or len(word) <max_word_len:\n",
    "                metrics += 1\n",
    "                # adding word or its synonym to the sentence\n",
    "                if uniform() < change_to_synonym_chance:\n",
    "                    try:\n",
    "                        synonyms, _ = get_synonyms_and_antonyms(word)\n",
    "                        new_sentence +=  random.choice(synonyms)  \n",
    "                    except:\n",
    "                        new_sentence += word + ' '\n",
    "                else:\n",
    "                    new_sentence += word + ' '\n",
    "\n",
    "        # adding sentence if it meets requeirments\n",
    "        metrics = metrics/len(sentence_to_words)\n",
    "        new_sentence = new_sentence[:-1]\n",
    "        matches = tool.check(new_sentence)\n",
    "        for match in matches:\n",
    "            if 'anise' in match.replacements:\n",
    "                matches.remove(match)\n",
    "        new_sentence = language_check.correct(new_sentence, matches)\n",
    "        if metrics >= sentence_quality:\n",
    "            cleaned_article.append(new_sentence)\n",
    "    cleaned_articles.append(deepcopy(cleaned_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random new article\n",
    "# splitting each article into introduction, elaboration and conclusion and mixing them \n",
    "random_article = {'Introduction': [], 'Elaboration': [], 'Conclusions': []}\n",
    "\n",
    "for article in cleaned_articles:\n",
    "    if len(article) > 10:\n",
    "        introduction = article[:int(len(article)/6)]\n",
    "        elaboration = article[int(len(article)/6):int(len(article)*5/6)]\n",
    "        conclusions = article[int(len(article)*5/6):]\n",
    "        split_1 = random.randint(1, len(introduction))\n",
    "        split_2 = random.randint(1, len(elaboration))\n",
    "        random_article['Introduction'] += introduction[int(split_1/2) : split_1]\n",
    "        random_article['Elaboration'] += elaboration[int(split_2/2): split_2]\n",
    "        random_article['Conclusions'] += conclusions[int(split_1/2) : split_1]\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO crossover tekstu, synonimy, .docx, zdjecia, sensowna struktura tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data science - if you would like to know dictonary of your article\n",
    "word_count = 0\n",
    "word_dict = {}\n",
    "for key in random_article.keys():\n",
    "    for sentence in random_article[key]:\n",
    "        word_count += len(re.split(' |\\n',  sentence))\n",
    "        for word in re.split(' |\\n',  sentence):\n",
    "            if word in word_dict.keys():\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1 \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(text, lenght):\n",
    "    # function that creates header for paragraph \n",
    "    # deleting stopwords\n",
    "    text = ' '.join([word for word in text.lower().split() if word not in (nltk.corpus.stopwords.words('english'))])\n",
    "    # creating dictionary with counts for each word\n",
    "    dictonary = {}\n",
    "    for word in re.split('[ ?!,.]', text):\n",
    "        if len(word) > 4:\n",
    "            new_word = True\n",
    "            for key in dictonary.keys():\n",
    "                if word == key:\n",
    "                    new_word = False\n",
    "                    dictonary[word] += 1\n",
    "                    break\n",
    "                if word in key:\n",
    "                    new_word = False\n",
    "                    dictonary[word] = dictonary.pop(key)\n",
    "                    break\n",
    "                if key in word:\n",
    "                    new_word = False\n",
    "                    break\n",
    "            if new_word:\n",
    "                dictonary[word] = 1\n",
    "    # first we add most frequent words        \n",
    "    header = ''\n",
    "    for index, word in enumerate(sorted(dictonary, key=dictonary.get, reverse=True)):\n",
    "        header += word\n",
    "        if index ==  lenght:\n",
    "            header = header[:1].upper() + header[1:]\n",
    "            # next we try to create a sensical sentence from it\n",
    "            matches = tool.check(header)\n",
    "            return language_check.correct(header, matches)\n",
    "            \n",
    "        header += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0bonus.jpg\n10bonus.jpg\n11bonus.png\n12bonus.png\n13bonus.png\n14bonus.png\n15bonus.png\n16bonus.png\n1bonus.jpg\n2bonus.png\n3bonus.png\n4bonus.png\n5bonus.jpg\n6bonus.png\n7bonus.png\n8bonus.jpg\n9bonus.jpg\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\nlist index out of range\n"
    }
   ],
   "source": [
    "# creation of .docx\n",
    "document = Document()\n",
    "document.add_heading(topic.upper(), 0)\n",
    "\n",
    "m = len(os.listdir('./photos/'))\n",
    "n = sum([len(random_article[key]) for key in random_article.keys()])\n",
    "if n > m:\n",
    "    r = m/(n * 2)\n",
    "else:\n",
    "    r = n/(m * 2)\n",
    "image_chance = 1\n",
    "\n",
    "for key in random_article.keys():\n",
    "    split_index = random.randint(10,20)\n",
    "    part = ''\n",
    "    for index, sentence in enumerate(random_article[key]):\n",
    "        part += sentence + '. '\n",
    "        if index == split_index:\n",
    "            document.add_heading(get_header(part, random.randint(1,3)), 2)\n",
    "            paragraph = document.add_paragraph()\n",
    "            paragraph.add_run(part)\n",
    "            split_index += random.randint(10,15)\n",
    "            part = ''\n",
    "        image_chance -=  r\n",
    "        if uniform() > image_chance:\n",
    "            try:\n",
    "                filename = os.listdir('./photos/')[0]\n",
    "                print(filename)\n",
    "                try:\n",
    "                    document.add_picture('./photos/' + filename)\n",
    "                except:\n",
    "                    pass\n",
    "                image_chance = 1\n",
    "                os.remove('./photos/' + filename)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                continue\n",
    "\n",
    "            \n",
    "    document.add_heading(get_header(part, random.randint(2,4)), 2)\n",
    "    paragraph = document.add_paragraph()\n",
    "    paragraph.add_run(part)\n",
    "document.save('report.docx')\n",
    "shutil.rmtree('./photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = re.sub('(figure|fig|fig.) [0-9]', '' ,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbasecondaad1c03b1f70f4dd78a0b9f5ad16532ea",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}